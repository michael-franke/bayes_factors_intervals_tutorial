---
title: "Meaningful results for meaningful hypotheses: A tutorial on hypothesis testing with Bayes factors using ROPEs"
shorttitle: "Meaningful results for meaningful hypotheses"
number-sections: true
author:
  - name: Timo B. Roettger
    corresponding: true
    orcid: 0000-0003-1400-2739
    email: timo.roettger@iln.uio.no
    affiliations:
      - name: University of Oslo
        department: Department of Linguistics & Scandinavian Studies
        city: Oslo
        country: Norway
  - name: Michael Franke
    corresponding: false
    #orcid: 0000-0003-1400-2739
    email: michael.franke@uni-tuebingen.de
    affiliations:
      - name: University of Tübingen
        department: Department of Linguistics
        city: Tübingen
        country: Germany
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: The authors have no conflict of interest to declare.
    financial-support: null
    gratitude: null
abstract: "Recent times have seen an increase of interest in Bayesian inference across the behavioral sciences. However, the process of testing hypotheses is often conceptually challenging or computationally costly. This tutorial provides an accessible, non-technical introduction to a technique that is both conceptually easy to understand and computationally cheap, and that also covers many common scenarios in the experimental sciences: Quantifying the relative evidence for a pair of interval-based hypotheses using Bayes factors through the Savage Dickey approximation."
keywords: [statistics, Bayes, Bayes factor, Savage Dickey, hypothesis testing, ROPE]
bibliography: bibliography.bib
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: doc
    keep-tex: true
    floatsintext: true
    include-in-header:
      text: |
        \AtBeginDocument{%
          \setcounter{topnumber}{4}
          \setcounter{bottomnumber}{2}
          \setcounter{totalnumber}{6}
          \renewcommand{\topfraction}{0.95}
          \renewcommand{\bottomfraction}{0.80}
          \renewcommand{\textfraction}{0.07}
          \renewcommand{\floatpagefraction}{0.6}
          \floatplacement{figure}{!tbp}% optional global nudge
          \setlength{\parindent}{5pt}
        }
    fig-pos: "!tbp"
#header-includes: \usepackage{annotate-equations}
execute:
  echo: true
  warning: false
  message: false
  error: false
  cache: false
editor: 
  markdown: 
    wrap: sentence
---

# Introduction

\noindent
One of the most common scenarios in experimental research is to measure one or more outcomes (dependent variables) in an experiment with one or more predictors (independent variables).
Usually, if we are testing hypotheses, we want to statistically test whether the predictors affect the measured variables.
Traditionally, these statistical tests have been done within the *null hypothesis significance testing* (NHST) framework.[^NHST]
While extensions of the NHST framework exist, in its basic form, NHST only allows us to *reject* the null hypothesis, but not to provide evidence in favor of it.
Over the last decade or so, however, there has been rising interest in statistical approaches within an alternative inferential framework using *Bayesian inference*.
One of the main reasons for this rising interest is that Bayesian inference allows to not only quantify evidence *against* an assumed null hypothesis, but also to yield quantitative evidence *in favor of* the null hypothesis.

[^NHST]: Not strictly necessary for this tutorial but, in case you need a reminder, the logic of NHST goes something like this: we assume ---for the sake of argument--- that a null hypothesis is correct, i.e., that there is no effect of a relevant predictor. We then ask ourselves how likely different observations would be based on that assumption, and use this so-called *sampling distribution* to quantify how surprising the observed data is under the assumed null hypothesis.
If the observed data are very unlikely, we *reject* the null hypothesis and conclude that the predictor affects the dependent variable.


Unfortunately, there are several approaches to hypothesis testing within the Bayesian framework, and many of them are either conceptually challenging, computationally (too) costly, or both.
For example, there are good conceptual arguments that support Bayesian hypothesis testing through *model comparison* using Bayes factors [@KassRaftery1995-Bayes-Factors;@VandekerckhoveMatzke2013-Model-Compariso;@MoreyRomeijn2016-philosophyOfBFs], but the computation of Bayes factors can be quite costly, especially for complex models.
Yet, for some of the most common use cases, there are some simple and computationally cheap approaches to Bayesian hypothesis testing with Bayes factors that are easy to understand and implement.
One such method is the *Savage-Dickey density ratio* [@DickeyLientz1970-The-Weighted-Li;@WagenmakersLodewyckx2010-Bayesian-hypoth].
While prior work has prominently documented how to use this method for the case of point-valued null-hypotheses [@WagenmakersLodewyckx2010-Bayesian-hypoth], this method can be hard to estimate reliably with posterior sampling, which is the most prevalent method for approximating Bayesian computation at the moment. 
This tutorial therefore focuses on the use of the Savage-Dickey density ratio for testing hypotheses that are grounded in *regions of practical equivalence* (ROPEs) [@kruschke_Rejecting_journalarticle_2018] using the so-called *encompassing priors* approach [@KlugkistKato2005-Bayesian-model;@KlugkistHoijtink2007-The-Bayes-facto;@Oh2014-Bayesian-compar;@WetzelsGrasman2010-An-encompassing], which is both conceptually more meaningful and computationally more robust than point-valued hypothesis testing.
While this method does not seem to be widely known, it is conceptually simple and easy to apply, e.g., through implementation in the package `bayesfactorR`.
This tutorial therefore provides an accessible, non-technical introduction to this method of Bayesian hypothesis testing, which is easy to understand, computationally cheap and widely applicable.

# Motivation and intended audience

\noindent
This tutorial provides a very basic introduction to hypothesis testing with Savage-Dickey density ratios using R (R Core Team, 2025).
We wrote this tutorial with a particular reader in mind.
If you have used R before and if you have a basic understanding of linear regression and Bayesian inference, this tutorial is for you.
We will remain mostly conceptual to provide you with an accessible tool to approach hypothesis testing within Bayesian inference.
The form of hypothesis testing that we would like to introduce to you is, however, different from the traditional null hypothesis significance testing in that it requires more thinking about the quantitative nature of your data.
This is not a bug but, at least for us, a feature that will allow you to understand both your data and what you can learn from them better.

If you don’t have any experience with regression modeling, you will probably still be able to follow, but you might also want to consider doing a crash course.
To bring you up to speed, we recommend the excellent tutorial by Bodo @winter_Linear_preprint_2013 on mixed eﬀects regression in a non-Bayesian paradigm.
To then make the transition to Bayesian versions of these regression models, we shamelessly suggest our own tutorial on "Bayesian Regression for Factorial Designs" [@franke-roettger_Bayesian_preprint_2019] which uses the same example data set as Winter's tutorial.
In a sense, the present tutorial on hypothesis testing could be considered the long-awaited sequel (!?) of the series started by Winter.

To actively follow this tutorial, you find all code and data in this repository: https://github.com/michael-franke/bayes_factors_intervals_tutorial.
You should have R [@R_program] installed on your computer (https://www.r-project.org).
Unless you already have a favorite editor for tinkering with R scripts, we recommend to try out RStudio (https://www.rstudio.com).
You will also need some packages, which you can import with the following code:

```{r setup}
#| echo: FALSE

# set the random seed in order to make sure
# you can reproduce the same results
set.seed(1702)

# project colors
project_colors = c(
  "#7581B3", "#99C2C2", "#C65353", "#E2BA78", "#5C7457", "#575463",
  "#B0B7D4", "#66A3A3", "#DB9494", "#D49735", "#9BB096", "#D4D3D9",
  "#414C76", "#993333"
  )

```

```{r libraries}
#| message: FALSE
#| echo: FALSE

# package for convenience functions (e.g. plotting)
library(tidyverse)
library(ggdist)

# package for Bayesian regression modeling
library(brms)

# package for posterior wrangling and plotting
library(tidybayes)
library(patchwork)

# package for BF calculation and plotting
library(bayestestR)

# options to increase efficiency of brms models (optional)
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())

```

```{r libraries_for_reader}
#| message: FALSE
#| eval: FALSE

# package for Bayesian regression modeling
library(brms)

# package for BF calculation and plotting
library(bayestestR)

```

# Data, research questions & hypotheses

\noindent
In this section, we introduce the data set that we will use throughout this tutorial, the research question that we want to address, and how to formulate meaningful hypotheses in a way that allows us to test them with Bayes factors using so-called Regions of Practical Equivalence (ROPEs), to be introduced below.

## The data set: Voice pitch in Korean across social contexts

\noindent
This tutorial looks at a data set relevant for investigating whether voice pitch diﬀers across social contexts in Korean.
Korean is a language in which the social distance between speakers plays a central role to the way utterances are pronounced.
The way Korean speakers talk depends for example on whether they are in a formal context (e.g. during a job interview) or an informal context (e.g. chatting with a friend about the holidays).
To investigate this phenomenon, our data set contains pitch measurements of utterances in different social contexts [@winter-grawunder_Phonetic_journalarticle_2012].
To load and inspect the data into your R environment, run the following code:

```{r load-data}
#| message: FALSE

polite <- 
  
  # load data set & cast strings to factors
  read_csv("https://tinyurl.com/yu5zskbp", col_types = "ffffd") 

head(polite)

```

This data set contains anonymous identifiers for 16 individual speakers stored in the variable `subject`. 
Voice pitch is dependent on speakers' `gender`, which we need to take into account as well.
Speakers produced 7 diﬀerent `sentences`, and the experiment manipulated whether the sentences were produced in a `formal` or an `informal` social `context`. 
Crucially, each row contains a measurement of pitch in Hz stored in the variable `pitch`.

For most analyses of behavioral experiments, researchers are interested in whether an outcome variable is meaningfully affected by at least one manipulated variable and if so how the outcome variable is affected by it.
In this case, @winter-grawunder_Phonetic_journalarticle_2012 wanted to test whether voice pitch is meaningfully affected by the social context of the utterance.

As a first step, we can explore this question visually.
@fig-descriptive-dataviz displays the pitch values for all utterances in the dataset across contexts (semi-transparent points).
The solid points indicate the average pitch values across all sentences and speakers.
Looking at the plot, we can see that voice pitch from utterances in formal contexts are on average slightly lower than those in informal contexts:
The red distribution is slightly shifted to the left of the blue distribution by 10-ish Hz for male speakers and 30-ish Hz for female speakers.
In other words, speakers tend to slightly lower their voice pitch when speaking in a formal context.
But there is also a lot of overlap between the two contexts.
Now as Bayesians, we would like to translate the data into an expression of evidence: Does the data provide evidence for our research hypotheses?

```{r descriptive-dataviz}
#| message: FALSE
#| warning: FALSE
#| echo: FALSE
#| fig-height: 2
#| fig-cap: "Empirical distribution of speakers' pitch values across contexts and sex"
#| fig-pos: "!tbph"
#| label: fig-descriptive-dataviz

polite |> 
  # aggregate mean values for context
  group_by(context, gender) |> 
  summarize(pitch = mean(pitch, na.rm = TRUE)) |> 
  ggplot(aes(y = context, 
             x = pitch, 
             fill = context,
             colour = context)) + 
  # plot all data as semitransparent points
  geom_point(data = polite,
             position = position_dodge(0.5), 
             alpha = 0.5, 
             size = 3) +
  # plot mean values per condition as large points
  geom_point(position = position_dodge(0.5), 
             pch = 21, 
             colour = "black",
             size = 5) +
  #scale_x_continuous(limits = c(10,40)) +
  scale_colour_manual(breaks = c("informal", "formal"),
                      values = c(project_colors[1], project_colors[3])) +
  scale_fill_manual(breaks = c("informal", "formal"),
                    values = c(project_colors[1], project_colors[3])) +
  facet_grid(~gender, scale = "free") +
  labs(x = "\npitch in Hz",
       y = "\n") +
  theme_minimal() +
  theme(legend.position = "none",
        panel.border = element_rect(color = "lightgrey", fill = NA, linewidth = 1))

```

## A Bayesian regression model to address our research question

\noindent
Let us build a Bayesian linear model to approach an answer to this question.
Using the package `brms` [@Burkner2018-Advanced-Bayesi], our first step is to specify the model formula and check which priors need to be specified:

```{r model_prep}
#| output: true

# contrast code predictors
contrasts(polite$context) <- c(-0.5,0.5)
contrasts(polite$gender) <- c(-0.5,0.5)

# define linear model formula
# predict pitch by context and gender
# and allow for context to vary between subjects and sentences
formula <- bf(pitch ~ context + 
                      gender + 
                      (1 + context | subject) +
                      (1 + context | sentence))

# get information about priors that are set per default for this model
# NB: no prior for `context1` is set per default (!)
as_tibble(get_prior(formula, polite)) |> 
  select(class, prior) |> filter(prior != "")

```

The default priors that `brms` picks for the Intercept and the variance parameters are mostly reasonable as they are derived from the data. They are weakly informative and symmetrical.
However the default choice for our critical parameter `context1` is to not specify a prior at all, i.e., to assume a flat (improper) prior, so that it is not even listed in the table above. 
Yet, there are good arguments why it should also receive a weakly informative prior [@gelman-etal_Prior_journalarticle_2017], i.e., the prior assumption about the difference between informal and formal contexts should be that we don't know, but our best guess is that it is zero in expectation and equally likely to be more or less than zero.
So we specify a normal distribution centered on zero for this parameter (and we do the same for gender).
Since we used contrast coding, the prior for `context1` reflects our prior belief about the difference between formal and informal contexts.
Note that we use default priors for the other parameters for convenience here, but you should always critically reflect on all of your priors.

```{r priors-2}

# define a weakly informative prior for context and gender
priors <- c(prior(normal(0, 20), coef = "context1"),
            prior(normal(0, 100), coef = "gender1"))

```

Now we inspect how the parameter distribution looks like *before* having seen the data, based on the priors only.
This is a useful exercise to make sure that the priors result in reasonable quantitative assumptions.
We usually do it for all parameters, but here we will focus only on the critical parameter `context1`, i.e., the difference between formal and informal contexts.
Let us also have a look at the predictions for the prior-only model.

```{r priors-model}
#| warning: FALSE
#| message: FALSE
#| output: FALSE
#| 
# run the prior-only model
fit_prior <- brm(formula, prior = priors, data = polite,
           # sample prior only
           sample_prior = "only",
           # common sampling specifications
           seed = 1234, iter = 8000)
           
```

```{r plot-priors}
#| echo: FALSE
#| fig-height: 3
#| fig-cap: "Prior probability of the difference in pitch between contexts, i.e., before seeing the data"
#| label: fig-plot-priors

# extract prior samples
prior_samples <- 
  fit_prior |> 
  spread_draws(b_context1)
  
# plot  
ggplot(prior_samples,
       aes(x = b_context1)) +
  geom_step(
    stat = "bin", bins = 100,
    lwd = 2,
    color = project_colors[3],
    direction = "mid"
  ) +
  geom_histogram(
    bins = 100, alpha = 0.3, 
    colour = NA, fill = alpha(project_colors[3], 0.5),
    position = "identity"
  ) +
  labs(x = "\n pitch difference between formal and informal contexts",
       y = "") +
  scale_x_continuous(limits =c(-75,75)) +
  theme_minimal() +
  theme(axis.text.y = element_blank())

```

Looking at the distribution in @fig-plot-priors, the prior for the effect of context on pitch seems sensible.
The most plausible value is zero.
Values that are smaller or larger than zero become less plausible the further they are away from zero and values being smaller or larger than zero are equally likely.
Good.
Before we have seen the data, our model is somewhat pessimistic about the effect of context on pitch.
Now we can run the full model that integrates the likelihood (our data) with the priors and visualize the posteriors for the critical parameter.

```{r model-1}
#| warning: FALSE
#| message: FALSE
#| output: FALSE

# run the model
fit <- brm(formula, prior = priors, data = polite,
           seed = 1234, iter = 8000)
           
```

```{r plot-posterior}
#| warning: FALSE
#| message: FALSE
#| echo: FALSE
#| fig-height: 4
#| fig-cap: "Posterior probability of the effect of context on pitch, i.e., after seeing the data"
#| label: fig-plot-posterior


# extract posterior samples
posterior_samples <- 
  fit |> 
  spread_draws(b_context1)
  
# plot prior and posterior
posterior_plot <- posterior_samples |> 
  ggplot(aes(x = b_context1)) + 
  geom_step(data = prior_samples,
    stat = "bin", bins = 100,
    lwd = 2,
    color = project_colors[3],
    direction = "mid"
  ) +
  geom_histogram(data = prior_samples,
    bins = 100, alpha = 0.3, 
    colour = NA, fill = alpha(project_colors[3], 0.5),
    position = "identity"
  ) +
  geom_step(stat = "bin", bins = 100,
    lwd = 2,
    color = project_colors[10],
    direction = "mid"
  ) +
  geom_histogram(bins = 100, alpha = 0.3, 
    colour = NA, fill = alpha(project_colors[10], 0.5),
    position = "identity"
  ) +
  scale_thickness_shared() +
  labs(x = "\n pitch difference between formal and informal contexts",
       y = "") +
  scale_x_continuous(limits =c(-75,75)) +
  theme_minimal() +
  theme(axis.text.y = element_blank())

posterior_plot

```

@fig-plot-posterior shows the prior (red distribution) and posterior (gold distribution) probability of the effect of context on pitch.
The distribution of posterior samples suggests that the majority of plausible values after seeing the data are positive, or in other words, informal contexts elicit larger pitch values.
Negative values are not very plausible under the posterior distribution, but also not completely implausible.
Compared to our prior probability (red distribution) for which roughly 50% of posteriors are negative, this decrease in plausibility of negative values is quite noteworthy already.

What we have done here should be quite familiar. 
We care about whether values are positive or negative because we compare our model predictions to a reference point: the single point value zero.
But do we really care that much for such point hypotheses?
Is zero really that special?
We might think so because years of using null hypothesis significance testing has conditioned us to think that way.
But we can also go beyond point-values if we see reason to do so.

## Grounding hypotheses in Regions of Practical Equivalence

\noindent
Above we claimed that we wanted to test "whether pitch is **meaningfully affected** by the social context of the utterance".
We snuck the word "meaningfully" in there for a reason.
But what does "meaningful" mean?
This is an interesting yet deep questions and (un)fortunately requires some thinking.
What a meaningful difference really constitutes depends on the context of the data. 
So let's have a closer look at our data.

This tutorial deals with speech data.
Speech is, in spoken languages at least, *the* vehicle to transmit linguistic information in order to communicate with each other.
Speech is also very complex and very noisy: Not everything that can be measured in the acoustic signal matters for a listener.
For example, if something cannot be perceived reliably, it is at least conceivable that it might play little to no role in communication.
While the speech sciences have a rich research tradition to estimate what can and what cannot be reliably heard, exact estimation depends on a lot of moving parts.
Approximate thresholds of what can be reliably heard are often referred to as *Just Noticeable Differences* (JNDs).
This terminology has been argued to be dangerously misleading [@SanfordHalberda2023-A-Shared-Intuit] as it implies a hard and absolute threshold of perceptibility. However, many researchers work with a more lenient and more practical conceptualization of a threshold: The JND is traditionally defined as the point at which the probability that our perceptual system registers a stimulus or a stimulus difference reaches as certain value. By convention, the JND is often defined as the point where listener accuracy exceeds the arbitrary threshold of 75%.
When we say "just noticeable difference" in the following we mean that latter probabilistic notion, as we would like to use the idea of regions at which the perceptual system is unlikely to reliably register a stimulus difference as a means of justifying a Region of Practical Equivalence (ROPE).
For example, while classic work by @klatt1973discrimination suggest JNDs ranging from 0.3 to 4 Hz, more modern treatments such as @turner2019perception report on JNDs between 17 and 25 Hz for non-speech stimuli and between 35 and 40 Hz for speech stimuli. 
While these studies are hard to compare, their effect magnitudes are grounded in a comparable probability threshold (75% accuracy). Thus, they give us at least an idea about the rough order of magnitude for JND values to work with when it comes to speech data like the data set at hand. 

Based on these considerations, we could interpret the original hypothesis the following way: If a pitch difference is below the JND, it is not (practically) meaningful.
So, instead of testing against a point-valued hypothesis, we can test against a range of parameter values that are equivalent to the null value for practical purposes.
In our case, let us begin with a JND value that is somewhat conservative in relation to @klatt1973discrimination and somewhat liberal in relation to @turner2019perception: We assume that pitch values between `-10` and `10` are negligible. 
Bear with us, we will later revisit this assumption.
Said differently, in order to be convinced that a difference in pitch is meaningful, it should be reliably greater than 10 Hz.
Such ranges are sometimes called *Regions of Practical Equivalence* (ROPEs), range of equivalence, equivalence margin, smallest effect size of interest, or good-enough belt [see @kruschke_Rejecting_journalarticle_2018].

```{r rope}

# define our ROPE
rope <- c(-10,10)

```

With a ROPE being defined, we can now test our hypothesis "whether pitch is **meaningfully affected** by the social context of the utterance" using Bayes factors.

# Testing hypotheses using Bayes factors

## What are Bayes factors?

\noindent
We often consider two hypotheses $H_0$ and $H_1$, and want to know which of these is correct.
We do so by looking at some observed data $D$.
As Bayesians, the first most obvious thing to look at is how likely each hypothesis is after seeing the data, i.e., something like $P(H_0 \mid D)$ and $P(H_1 \mid D)$.
Now, it turns out that these *posterior probabilities of hypotheses* are problematic, because they depend on the prior probabilities of the hypotheses $P(H_0)$ and $P(H_1)$, which are often hard to justify.
To see this, imagine that the hypotheses to compare are polarizing issues like contrasting  Darwinian evolutionary theory and a dull form of creationism, referred to here as *arbitrary design*.
Proponents of either view would have a hard time agreeing on priors for these hypotheses, but may find it much easier to agree on whether a given observation $D$ is more likely under the assumption that one of the two hypotheses is correct, rather than the other.
Therefore, Bayes factors are defined as the *likelihood ratio* of the data given each hypothesis:

$$
\text{Bayes factor in favor of hypothesis 1 over hypothesis 0} \ \  \colon\!= \ \ \frac{P(D \mid H_1)}{P(D \mid H_0)}
$$

To see how this is a more objective and actually quite intuitive measure of observational evidence in scientific reasoning, consider the case of Darwinian evolution ($H_1$) versus arbitrary design ($H_0$) again.
Let's assume that the observed data $D$ consist of (i) measurements of the beak shapes of finches on two different islands, and (ii) information about the food sources available on these islands.
Let's assume for simplicity that on island A the predominant food source are insects found deep in wood or earth, and that on island B it is insects with hard shells living on the surface.
We also observe that finches on island A have long and narrow beaks, while finches on island B have short and thick beaks.
(This here is a crude simplification of an actual case that led Darwin to formulate his theory, but bear with us for the sake of the example.)
What is a better explanation of data $D$, evolutionary selection or arbitrary design?
To begin with, let's notice that $D$ is *not* ruled out by either hypothesis.
But the probability of observing $D$ is higher under Darwinian evolution ($H_1$) than under arbitrary design ($H_0$).
This is because, before seeing the data, a proponent of evoluationary theory $H_1$, if asked to make a prediction about beak shapes under the given food sources, would have considered it *more* likely that beaks are adapted to their function of exploiting the dominant food source, so making the actually observed data $D$ more likely *ex ante* than at least some other possible observations.
In contrast, a proponent of arbitrary design $H_0$ would not have had any reason to expect that beak shapes are adapted to food sources, and would like to be able to rationalize *ex post* also any apparent violation of this expectation as the inscrutable way of the arbitrary designer.
To the extent that there are many alternative observations which arbitrary design would consider reasonably likely *ex ante* but evoluationary selection would rule out (or deem very unlikely), the probability of the observed data is much higher under Darwinian evolution than under arbitrary design, so that $P(D \mid H_1) > P(D \mid H_0)$, irrespective of what we initially believed is the more plausible hypothesis.
This is what corroborates the intuition that the observation $D$ is an argument in favor of $H_1$ over $H_0$. This intuition is exactly what the Bayes factor quantifies.

Concretely, a Bayes factor of 1 corresponds to the case of $P(D \mid H_1) = P(D \mid H_0)$, i.e., the data is equally likely under both hypotheses, so the data does not provide any evidence for or against either hypothesis.
Any Bayes factor larger than 1 indicates that the data is more likely under $H_1$ than under $H_0$, and the larger the Bayes factor, the stronger the evidence in favor of $H_1$.
Conversely, any Bayes factor smaller than 1 indicates that the data is more likely under $H_0$ than under $H_1$.
Notice that the Bayes factor is symmetric in the sense that a Bayes factor of 3 in favor of $H_1$ over $H_0$ corresponds to a Bayes factor of 1/3 in favor of $H_0$ over $H_1$.
There are various conventions for interpreting the strength of evidence of Bayes factors, such as to consider Bayes factors smaller than 3 as "*anecdotal evidence*"; Bayes factors bigger than 3 as "*moderate evidence*"; and Bayes factors bigger than 10 as "*strong evidence*".

One way to interpret Bayes factors in absolute terms is this:
A Bayes factor of $n$ in favor of $H_1$ over $H_0$ means that after seeing the data, a rational researcher who initially thought both hypotheses were equally likely would consider $H_1$ to be $n$ times more likely than $H_0$ after observing $D$.

## Bayes factors for statistical models

\noindent
After motivating Bayes factors in general, let's have a look at the definition of Bayes factors in the context of statistical models in this section. 
What follows in this section is a bit more technical, so you can skip ahead without missing out too much information for applying these methods. 

In the context of statistical models, we can use Bayes factors to compare two statistical models $M_0$ and $M_1$ that instantiate two competing hypotheses (or assumptions) $H_0$ and $H_1$.
A Bayesian statistical model $M$ consists of:

1. a *likelihood function* $P(D \mid \theta, M)$ that specifies how likely the observed data $D$ is given the model $M$ and the model's parameters $\theta$, and 
2. a *prior distribution* $P(\theta \mid M)$ that specifies how likely different parameter values are before seeing the data.

The probability of some observed data $P(D \mid M)$ under a model $M$ is then obtained by integrating over all possible parameter values $\theta$:

$$
P(D \mid M) = \int P(D \mid \theta, M) \ \  P(\theta \mid M) \ \text{d} \theta
$$

This is called the *marginal likelihood* of the data under the model $M$.
We can think of this quantity as obtained from sampling parameter values from the prior and then sampling, for each of the sampled parameter values, a potential data observation.
(Notice that this is the *prior predictive data distribution* of the model.)

Putting things together, the resulting definition for Bayes factors in statistical models is:

$$
\text{Bayes factor in favor of model 1 over model 0} \ \  \colon\!= \ \ \frac{P(D \mid M_1)}{P(D \mid M_0)} = \frac{\int P(D \mid \theta, M_1) \ \  P(\theta \mid M_1) d\theta}{\int P(D \mid \theta, M_0) \ \  P(\theta \mid M_0) d\theta}
$$

## Bayes factor for point-valued hypotheses (the Savage-Dickey method)

\noindent
While Bayes factors are a very intuitive and useful measure of evidence, they are often hard to compute.
There are various approximation methods, such as bridge sampling [@GronauSarafoglou2017-A-tutorial-on-b], which can be used for any arbitrary pair of models, but these can still be computationally costly and sometimes hard to implement.
However, for the special case of *nested models*, there is a simple and computationally cheap approximation method called the *Savage-Dickey density ratio* [@DickeyLientz1970-The-Weighted-Li;@WagenmakersLodewyckx2010-Bayesian-hypoth].

What are nested models? Intuitively speaking, model $M_0$ is nested in model $M_1$ if $M_0$ can be obtained from $M_1$ by setting one or more parameters to a specific value.
(More precisely, by conditioning on a specific value of one or more parameters.)
For example, take the regression model $M_1$ for the Korean speech data we introduced at the beginning of this tutorial. We suggested a normal distribution on the `context` coefficient as a prior.
A model $M_0$ nested under it would be one that is exactly like $M_1$ except that $M_0$'s prior for the `context` coefficient allows only one value, e.g., that the slope coefficient is equal to zero.
That model $M_0$ would then correspond to the (standard, point-valued) null hypothesis that there is no effect of `context` on `pitch`.
This process might sound familiar to people who have generated p-values for linear mixed effects models before. One way to check if a predictor significantly affects a dependent variable is by comparing a full model to a null model. 
The null model is the full model minus the critical predictor, which is clamped to a specific value, so to speak. 

So, suppose that $M_0$ is nested in $M_1$ by fixing a critical parameter $\theta^*$ to a specific value $x$.
Then, the Savage-Dickey density ratio states that the Bayes factor in favor of $M_1$ over $M_0$ can be computed as the ratio of the prior and posterior density of $\theta^*=x$ from $M_1$'s point of view:
$$
\text{Bayes factor in favor of model 0 over model 1} \ \  = \ \  \frac{P(\theta^*=x \mid D, M_1)}{P(\theta^*=x \mid M_1)}
$$

Let's unpack this.
First of all, this seemingly magical result is actually not that magical, but follows directly from the definition of Bayes factors and Bayes' theorem. Don't worry. We won't bother you with the derivation here.
But that means that in practice we do not have to calculate or approximate any integrals at all, but we can simply look at the more complex model $M_1$ and its prior and posterior parameter distributions, like we routinely do with `brms`, for example.
Look at the formula above: we would only need to run one model, $M_1$, and then look at the prior and posterior density of the critical parameter $\theta^*$ at the point value $x$.
The prior us usually easily determined because it is in our hands to specify it.
The posterior can by estimated from the samples that are returned by software like `brms` ...

... well, at least in principle. 
One problem here is that estimating $P(\theta^*=x \mid D, M_1)$ from posterior samples is fickle.
We can do it with some mathematical methods, but we may need a lot of samples and do some post-processing (e.g., using splines).
But one technical wrinkle is that posterior samples are less reliable for estimating densities at specific points, but are usually more reliable for estimating probabilities over wide-enough intervals of values.[^vanishing-measures]
Moreover, point-valued hypotheses may often not be that interesting or meaningful in practice anyway, as argued above.
Fortunately, there is a generalization of the Savage-Dickey density ratio that works for ranges of values, too!

[^vanishing-measures]: The problem is one of a class of so-called **vanishing measures problems**.
The probability of a point value is a probability *density*. 
Markov Chain Monte Carlo (MCMC) methods, which are used by `brms` and many other Bayesian software packages, generate samples from the posterior distribution.
It is extremely unlikely that you will ever get a sample that is exactly equal to the point value $x$.
So you would need to estimate something like the probabilities of a very small interval around $x$, and put that in relation to similarly small intervals for all other possible values of $\theta^*$ to get a reliable estimate of the density at $x$.

## Bayes factors for interval-based hypotheses (like ROPEs)

\noindent
The Savage-Dickey density ratio can be generalized to the case where the null hypothesis $H_0$ is not a point-valued hypothesis, but a hypothesis that the critical parameter $\theta^*$ lies in some interval $I_0$, such as our ROPE from above.
There are several different ways to define the alternative hypothesis $H_1$ in this case, but the most common one is to define it as the complement of $H_0$, i.e., that $\theta^*$ lies in the interval that contains all values that are not in $I_0$, so that:

$$
H_0 = \theta^* \in I_0 \ \ \ \ \ \ \ \  H_1 = \theta^* \not \in I_0
$$

An efficient way of computing Bayes factors for such a setting is to use the so-called *encompassing priors approach* [@KlugkistKato2005-Bayesian-model;@KlugkistHoijtink2007-The-Bayes-facto;@Oh2014-Bayesian-compar;@WetzelsGrasman2010-An-encompassing].
According to this approach, we consider an *encompassing model* $M_e$ that contains both the null and the alternative hypothesis as special cases.
Concretely, the encompassing model $M_e$ could be just a regression model like the model we used above for the Korean speech data, with a prior distribution on the critical parameter $\theta^*$, such as the normal distribution on the slope coefficient for `context`.
The null model $M_0$ would then be the nested model that is obtained from $M_e$ by conditioning on $\theta^* \in I_0$, and the alternative model $M_1$ would be the nested model that is obtained from $M_e$ by conditioning on $\theta^* \not \in I_0$.
An alternative intuition can be gained by visualizing this principle. In @fig-savage-dickey_TR, blue parts of the sampled distributions fall within the ROPE representing the null model $M_0$, grey parts fall outside the ROPE representing the alternative model $M_1$. We get a null and an alternative model for both the model before observing the data, and after observing the data.

Based on this setup, the Bayes factor in favor of $M_0$ over $M_1$ can be computed as the ratio of the posterior and prior odds of $\theta^*$ being in $I_0$ (blue parts of the distributions) versus being in $I_1$ (grey parts), where $I_1$ is the complement of $I_0$:

$$
BF_{01} = \frac{P(\theta \in I_{0} \mid D, M_{e})}{P(\theta \in I_{1} \mid D, M_{e})} \ \frac{P(\theta \in I_{1} \mid  M_{e})}{P(\theta \in I_{0} \mid M_{e})}
$$


```{r Savage-Dickey-visualization_alternative}
#| echo: FALSE
#| label: fig-savage-dickey_TR
#| fig-height: 3

prior_samples <- prior_samples |> mutate(model = "prior")
posterior_samples <- posterior_samples |> mutate(model = "posterior")
all_samples <- full_join(prior_samples,posterior_samples)

# plot density ropes  
density_rope <- all_samples |> 
  ggplot(aes(x = b_context1)) + 
  stat_halfeye(aes(fill = after_stat(x > -10 & x < 10)),
                 interval_colour = NA) +
  facet_grid(.~ factor(model, levels = c("prior", "posterior"))) +
  scale_fill_manual(values = c("grey","lightblue")) +
  scale_thickness_shared() +
  geom_vline(xintercept = c(rope[1], rope[2]),
             lty = "dashed") +
  labs(x = "\n pitch difference between formal and informal contexts",
       y = "") +
  scale_x_continuous(limits =c(-75,75),
                    breaks = c(-40,0,40)) +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        legend.position = "none")

proportion_rope <- all_samples |> 
  mutate(in_rope = ifelse(b_context1 < -10 | b_context1 > 10, "outside", "inside")) |> 
  group_by(model,in_rope) |> 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) |> 
  ggplot(aes(x = 1, y = freq, fill = in_rope)) +
  geom_bar(width = 0.1, 
           stat = "identity") +
  facet_grid(.~ factor(model, levels = c("prior", "posterior"))) +
  scale_fill_manual(values = c("lightblue","grey")) +
  labs(x = "\n amount of samples in\nand outside of ROPE",
       y = "") +
  theme_minimal() +
  theme(axis.text = element_blank(),
        legend.position = "none")

density_rope + proportion_rope + plot_layout(widths = c(2, 1))

```


## Manually calculating the Bayes factor for our ROPE hypothesis

\noindent
To calculate the Bayes factor for our ROPE hypothesis, we can use the formula above using samples from the prior and the posterior based on our encompassing model.
For the case of the Korean speech data, we already obtained prior samples above in the `fit_prior` model, and posterior samples in the `fit` model.
So, we can use these to extract the proportion of samples that fall inside and outside of our ROPE and do the calculations by hand.
Let's do this first.

```{r ROPE-BF-posterior-samples}

prior_ROPE <- fit_prior |>
  spread_draws(b_context1) |>
  summarise(prior_ROPE = mean(b_context1 >= rope[1] & b_context1 <= rope[2])) |>
  pull()

post_ROPE <- fit |> 
  spread_draws(b_context1) |>
  summarise(post_ROPE = mean(b_context1 >= rope[1] & b_context1 <= rope[2])) |>
  pull()

```

Using these numbers, we can now calculate the Bayes factor in favor of the null hypothesis that the effect of `context` on `pitch` is in the ROPE versus the alternative hypothesis that it is outside of the ROPE:

```{r}
BF_favoring_Null <- (post_ROPE / (1 - post_ROPE)) /
                    (prior_ROPE / (1 - prior_ROPE))
BF_favoring_Null
```

The Bayes factor in favor of the alternative hypothesis is simply the inverse of this number:

```{r}

BF_favoring_Alt <- 1 / BF_favoring_Null
BF_favoring_Alt

```

With a Bayes factor of around `{r} round(BF_favoring_Alt, 2)`, the data does not provide noteworthy evidence in favor of the alternative hypothesis that the effect of `context` on `pitch` is outside of the ROPE.

In @fig-savage-dickey_TR, the Bayes factor is the amount of change between (i) the ratio of samples *within* and *outside* the ROPE in the prior, and (ii) the same quantity for the posterior.
So, to see evidence in favor of the null hypothesis (the ROPE), we would want to see the ratio of points shift in favor of the points *inside* of the ROPE as we go from prior to posterior. 
In the plot above, this does not seem to be the case.
Rather, we see a small shift that *more* probability mass is located *outside* the ROPE for the posterior distribution as opposed to inside of it, as compared to the prior.
This is why, at least in direction, the BF tells us to favor the alternative hypothesis.
However, the shift is not so very pronounced, so that we would not speak of noteworthy evidence in favor of the alternative hypothesis, let alone strong or decisive evidence.

```{r plot-ropes}
#| warning: FALSE
#| message: FALSE
#| echo: FALSE
#| include: FALSE
#| fig-height: 4
#| fig-cap: "Prior and posterior probability of the effect of context on pitch relative to the ROPE (-0.1, 0.1)"
#| label: fig-plot-ropes

posterior_plot + 
  geom_vline(xintercept = c(rope[1], rope[2]),
             lty = "dashed")

```


## Calculating ROPE-ed Bayes factor with the `bayesfactorR` package

\noindent
Instead of doing these calculations by hand, we can more conveniently calculate the Savage Dickey ratio with the `bayesfactor_rope()` function from the `bayestestR` package [@MakowskiBen-Shachar2019-bayestestR].
The function takes as input the posterior and prior fit objects (you can also only provide the posterior fit, in which case the function will sample from the prior for you).
The function then computes the ratio for the specified rope for the specified parameter.
Notice that the method implemented in this package is slightly different from the naive one we used above, in that it uses a method that provides more stable and precise estimates for smaller sets of samples.
(The package uses logsplines.)

```{r BF_rope}
#| warning: FALSE
#| message: FALSE

BF_1 <- bayesfactor_rope(posterior = fit, 
                         prior = fit_prior,
                         null = rope, 
                         parameter = "b_context1")
BF_1

```

We obtain (almost) the same result in this way: the Bayes factor in favor of the alternative hypothesis for the given ROPE is around `{r} round(exp(BF_1$log_BF), 2)`.

## Sensitivity analysis for different priors and ROPEs

\noindent
Now as you probably have guessed already, all these probabilities are very much dependent on the priors of the model, so it is important to evaluate the robustness of our Bayes factor-based interpretation across a range of sensible priors.
And as long as we are not a 100% sure about what a meaningful difference is, we might as well explore the robustness of the Bayes factor across different ROPEs, i.e., in our case different JNDs.
We won't bore you with the code for that process, but you can follow it along in our scripts.
Let us explore the following ROPE intervals as informed by the two studies cited above on pitch perception: we test a range of ROPE intervals from 0.3 Hz to 40 Hz.
We also assume the following five prior values for the standard deviation of the critical parameter (centered on zero): 10, 15, 20, 25, 30.
These are all sensible prior widths assuming that medium to strong pitch effects in either direction are plausible.

```{r loop-priors-for-full-models}
#| echo: FALSE
#| message: FALSE
#| label: loop-priors-for-full-models

# pick a weakly informative prior for the control parameter gender
priors = prior(normal(0, 100), 
                 class = b, 
                 coef = "gender1")

# whether to rerun or load stored models
rerun <- FALSE

if (rerun) {
  
  # define 5 different priors widths that make sense
  
  priors_10 <- c(priors, prior(normal(0, 10), class = b, coef = "context1"))
  priors_15 <- c(priors, prior(normal(0, 15), class = b, coef = "context1"))
  priors_20 <- c(priors, prior(normal(0, 20), class = b, coef = "context1"))
  priors_25 <- c(priors, prior(normal(0, 25), class = b, coef = "context1"))
  priors_30 <- c(priors, prior(normal(0, 30), class = b, coef = "context1"))
  
  # Define a list of these prior specifications
  prior_list <- list(priors_10,
                     priors_15,
                     priors_20,
                     priors_25,
                     priors_30
  )
  
  # Initialize a list to store models
  model_list <- list()
  
  # Loop over the priors
  for (i in seq_along(prior_list)) {
      model_list[[i]] <-
        brm(
          formula = formula,
          data = polite,
          prior = prior_list[[i]],
          # common sampling specifications
          seed = 1234,
          iter = 8000,
    )
  }
  
  # name models
  names(model_list) <- paste0("xmdl_prior_", c(10,15,20,25,30))
  
  # store models
  saveRDS(model_list, "../models/model_loop.RDS")
  
} else {
  model_list <- readRDS("../models/model_loop.RDS")
}


```


```{r loop-through-ropes}
#| echo: FALSE
#| message: FALSE

# # specify different rope intervals
# 
# # Generate sequence of lower bounds
# lower_bounds <- c(c(0.3,1), seq(2, 40, by = 2))
# 
# # Create list of value pairs
# ropes <- lapply(lower_bounds, function(x) c(x, -x))
# 
# results <- map_dfr(names(model_list), function(model_name) {
#   model <- model_list[[model_name]]
# 
#   map_dfr(ropes, function(interval) {
#     lower <- interval[1]
#     upper <- interval[2]
# 
#     bayesfactor_parameters(
#       model,
#       null = c(lower, upper),
#       parameter = "b_context1"
#     ) %>%
#       as_tibble() %>%
#       mutate(
#         model = model_name,
#         ROPE = upper,
#         interval_range = paste0("[", lower, ", ", upper, "]")
#       ) %>%
#       select(model, everything())
#   })
# })
# 
# # wrangle
# results_wrangled <- results |>
#    filter(Parameter == "b_context1") |>
#    separate(model, sep = "_", into = c(NA, NA, "prior_sd")) |>
#    mutate(BF = exp(log_BF),
#           ROPE = abs(ROPE)) |>
#    select(prior_sd, BF, ROPE, interval_range)
# 
#  write_csv(results_wrangled, "models/loop_results.csv")
results_wrangled <- read_csv("../models/loop_results.csv")

```

```{r visualize-raster}
#| echo: FALSE
#| label: fig-sensitivity_plot
#| fig-height: 8
#| fig-cap: "Bayes factors for a range of priors and a range of ROPEs"

# plot raster
ggplot(results_wrangled,
       aes(x = as.factor(prior_sd),
           y = reorder(interval_range, -ROPE),
           fill = BF)) +
  geom_tile(colour = "grey") +
  geom_text(aes(label = round(BF,2)),
            size = 2.5) +
  scale_fill_gradient2(limits = c(0.03,10),
                       transform = "log",
                       midpoint = 1, 
                       mid = "white",
                       low = "#5C7457", 
                       high = "#FA8100",
                       n.breaks = 4,
                       breaks = c(0.00000,0.03,0.1,0.33,1,3,10),
                       labels = c("0",
                                  "0.03 > very strong for H0","0.1 > strong for H0",
                                  "0.33 > moderate for H0","1 = inconclusive",
                                  "3 < moderate for H1","10 < strong for H1"),
                       oob = scales::squish
                       ) +
  labs(title = "(A) Bayes factor in favour of alternative", 
       subtitle = "from moderate evidence against the null\nto very strong evidence for the null",
       fill = "Bayes factor\n",
       y = "ROPE in Hz centered on 0 by 2 Hz steps\n",
       x = "\nprior standard deviation in Hz") +
  theme_minimal()

```

The combination of Bayes factors is visualized in @fig-sensitivity_plot. Orange cells indicate evidence for the alternative.
Green cells indicate evidence for the null.
It becomes clear that the conclusions we can draw from our data are rather dependent on the choices we made along the way.

By comparing the Bayes factors along the y-axis, we can see that they are heavily dependent on the chosen ROPE.
We here chose (theoretically speaking) a quite large range of ROPEs, all of which are informed by psychoacoustic studies of what pitch differences can be reliably heard and thus likely are meaningful for communication. 
In light of this range of possible definitions of what constitutes meaningful differences, our data do not seem very robust, as illustrated by the shift from orange to green. Even the smallest ROPE intervals provide only anecdotal to moderate evidence for the alternative. And the most conservative ROPEs, following @turner2019perception, leads to moderate to very strong evidence against the alternative hypothesis. 

Additionally, when comparing the Bayes factors along the x-axis, we can see that they are comparatively consistent for different standard deviations of the critical prior.
However, we can also see that the Bayes factors decrease with the width of the priors (from left to right).
  * [ ] This is not surprising and a known phenomenon, often discussed under the Jeffreys-Lindley paradox [@lindley_Statistical_journalarticle_1957]: The more diffuse the priors are (i.e., wider priors), the more probability they put on extreme values that (usually) tend to make the data extremely unlikely.

Combined, we can see that the larger the ROPE and the wider the priors, the more likely becomes the null hypothesis.
In an ideal world, the evidence provided by the data should be robust across these choices. 

This exploration of our inference is a fantastic opportunity to assess the boundaries of our conclusions.
In this case, the conclusions of the original study by @winter-grawunder_Phonetic_journalarticle_2012 was based on the null hypothesis significance testing framework, traditionally testing the compatibility of the data with a point-null hypothesis.
Given his framework, it is reasonable to conclude that in formal speech, Korean speakers lower their average fundamental frequency.
However, thinking more deeply about the theoretical consequences of differences in pitch, it might be less clear that these differences are truly meaningful.
Given our proposed assumptions about what constitutes a meaningful effect, our analyses suggest that we neither find robust evidence for nor against the hypothesis that Korean speakers meaningfully lower their average fundamental frequency in formal speech.

# How to write this inferential procedure up?

\noindent
Here is a possible way to write up our analysis, following
Kruschke's catalog of best practices [@Kruschke2021-Bayesian-Analys]. We first have to describe our model structure, including the priors of all parameters, and then the inferential procedure combining ROPEs with Bayes factor. 

## Model structure

\noindent
The data were modeled using a hierarchical linear model predicting the continuous variable pitch (in Hz) by both the categorical predictor gender (male vs.
female, contrast-coded) and social context (informal vs.
formal, contrast-coded) and the maximal random-effects structure justified by the study's design [@barr2013random], including by-subject random slopes (n = 16), and by-sentence random slopes (n = 7) for social context.
Parameter estimation and inference is performed within the Bayesian framework.
The model was fit using `brms` [@Burkner2018-Advanced-Bayesi] in R [@R_program].
We used regularizing, weakly informative priors for the models [@gelman-etal_Prior_journalarticle_2017].
Concretely, we used a Student's *t* distribution for the prior for the intercept (df = 3, mean = 211.6, df = 80.2), corresponding to the grand mean of the empirical data, a Student's *t* distribution for the prior for all random effect variance components as well as residual variance (df = 3, mean = 0, df = 80.2), and a Lewandowski-Kurowicka-Joe distribution (LKJ, shape = 1) for all correlation parameters.
These priors were default priors, estimated from the data by brms.
We specified a reasonable weakly informative prior for the predictor gender (normal, mean = 0, sd = 100) and specified a range of reasonable weakly informative priors for the predictor of interest: social context (normal, mean = 0, sd = [10,15,20,25,30]).

We fit this model with four chains of Hamiltonian Monte Carlo sampling for the estimation of the joint posterior distribution using the No U-Turn Sampler as implemented in Stan [@carpenter2017stan], and 8000 iterations (of which 4000 for warm-up) per chain.


## Inferential assessment via Bayes factor and ROPEs

\noindent
Using the Bayesian framework, we aim to quantitatively evaluate the evidence for a perceptually meaningful effect of social context on pitch values based on our data against the background of our model and chosen priors.
We combine two statistical concepts to make this evaluation: First, we define a region of practical equivalence (ROPE) that represents a reasonable range of pitch values around zero that we consider to be not meaningful  [@kruschke_Rejecting_journalarticle_2018].
In our case, the ROPE is perceptually defined by studies on just noticeable differences in pitch perception [@klatt1973discrimination;@turner2019perception].

Subsequently, we calculate the Savage-Dickey density ratio [@DickeyLientz1970-The-Weighted-Li;@WagenmakersLodewyckx2010-Bayesian-hypoth], i.e., we relate the amount of evidence (the proportion of posterior samples) within the ROPE for the model based on the priors only (i.e., before seeing the data) to the amount of evidence within the ROPE for the model based on both priors and likelihood (i.e., after seeing the data) [e.g., @WetzelsGrasman2010-An-encompassing].
The Savage-Dickey method lets us assess evidence for and against a null hypothesis using Bayes factor (BF), since we are dealing with nested models.
Since BFs can depend on both the defined ROPE and the priors of the model, we assessed the sensitivity of the results through calculating BFs for a range of (sensible) ROPE values and a range of (sensible) priors.
We assumed the ROPE intervals centered on zero from 0.3 Hz to 40 Hz.
We assumed the following five prior values for the width of the context parameter (centered on zero): 10 Hz, 15 Hz, 20 Hz, 25 Hz, and 30 Hz.
These are all sensible prior choices assuming reasonable pitch differences in either direction.

Based on this setup, we consider each pair of ROPE and prior.
For each pair, we speak of moderate evidence for either hypothesis if the Bayes factor in its favor is as least 3.
The overall conclusion from this sensitivity analysis is a nuanced assessment of the interplay of ROPE and prior, drawing on empirical knowledge of the magnitudes involved (here: pitch).


# Some words of encouragement

\noindent
Bayesian inference in general and this form of hypothesis testing in particular require much more thinking than we might be used to.
We believe this is a good thing.
Many voices have criticized the lack of engagement that we behavioral scientists invest into thinking how our theoretical ideas connect to concrete predictions in the quantitative systems under investigation [@scheel2022most; @coretta2023multidimensional; @woensdregt2024lessons].
The presented form of hypothesis testing is easy to understand, but does require to think deeply about prior quantitative assumptions as well as what it means for observations to be meaningfully different.
That is neither trivial nor easy.
But we would like to encourage everybody to engage in exactly this thinking to better understand the relevant data in front of us and how it might link to our understanding of cognition and behavior.


# Other Resources

\noindent 
There are many fantastic resources out there to help you learn about the wonderful world of statistics in general and Bayesian inference in particular.
Here are a few recommendations.
A very accessible introduction to linear models in R, using a non-Bayesian frequentist approach, is [@winter2019statistics].
A good and gentle general first introduction to Bayesian statistics is [@KruschkeTextbook].
Another accessible, but slightly more technical introduction is [@Lambert2018-A-Students-Guid].
A fairly technical but very comprehensive introduction to Bayesian statistics is [@GelmanCarlin2014-Bayesian-Data-A].
If you already have some background in statistics, a great resource is [@McElreath2016-Statistical-Ret].
 


# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Session information

```{r sessionInfo}
#| echo: FALSE

sessionInfo()

c("brms", "bayestestR", "tidybayes", "tidyverse", "ggdist") %>%
  map(citation) %>%
  print(style = "text")

```

