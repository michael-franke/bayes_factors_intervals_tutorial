---
title: "Hypothesis Testing Using Bayes Factor in Behavioral Sciences"
shorttitle: "Test Bayes Factor"
number-sections: true
author:
  - name: Timo B. Roettger
    corresponding: true
    orcid: 0000-0003-1400-2739
    email: timo.roettger@iln.uio.no
    affiliations:
      - name: University of Oslo
        department: Department of Linguistics & Scandinavian Studies
        city: Oslo
        country: Norway
  - name: Michael Franke
    corresponding: false
    #orcid: 0000-0003-1400-2739
    email: michael.franke@uni-tuebingen.de
    affiliations:
      - name: University of Tübingen
        department: Department of Linguistics
        city: Tübingen
        country: Germany
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: The authors have no conflict of interest to declare.
    financial-support: null
    gratitude: null
    #authorship-agreements: Conceptionalization, Methodology, Validation, Formal Analysis, Review & Editing of Manuscript, Data Curation - TBR. & DLJE; Software, Investigation - DLJE; Writing of Original Draft, Visualization, Supervision - TBR.
abstract: "Recent times have seen a surge of Bayesian inference across the behavioral sciences. However, the process of testing hypotheses is often conceptually challenging or computationally costly. This tutorial provides an accessible, non-technical introduction that covers the most common scenarios in experimental sciences: Testing the evidence for an alternative hypothesis using Bayes Factor through the Savage Dickey approximation. This method is conceptually easy to understand and computatioanlly cheap."
keywords: [statistics, Bayes, Bayes Factor, Savage Dickey, hypothesis testing, ROPE]
bibliography: bibliography.bib
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: doc
    keep-tex: true
    floatsintext: true
    include-in-header:
      text: |
        \AtBeginDocument{%
          \setcounter{topnumber}{4}
          \setcounter{bottomnumber}{2}
          \setcounter{totalnumber}{6}
          \renewcommand{\topfraction}{0.95}
          \renewcommand{\bottomfraction}{0.80}
          \renewcommand{\textfraction}{0.07}
          \renewcommand{\floatpagefraction}{0.6}
          \floatplacement{figure}{!tbp}% optional global nudge
        }
    fig-pos: "!tbp"
#header-includes: \usepackage{annotate-equations}
execute:
  echo: true
  warning: false
  message: false
  error: false
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

# Introduction

One of the most common scenarios in experimental research is to measure one or several (dependent) variable in an experiment which with one or more predictors (independent variables).
Usually, we then want to test statistically whether the predictors affect the measured dependent variables.
Traditionally, these statistical tests have been done within the *null hypothesis significance testing* (NHST) framework.
The logic of NHST is to reason as follows: we assume ---for the sake of argument--- that a null hypothesis is correct, i.e., that there is no effect of the relevant predictors; then we ask our selves how likely different observations would be based on that assumption, and use this so-called *sampling distribution* to quantify how surprising the observed data is under the assumed null hypothesis.
If this the observed data is very unlikely, i.e., very surprising, we *reject* the null hypothesis and conclude that there is an effect of the predictors on the dependent variable.
While extensions of the NHST framework exist, in its basic form, NHST only allows to reject the null hypothesis, but not to provide evidence in favor of it.

Over the last decade or so, however, there has been rising interest in statistical approaches within an alternative inferential framework using *Bayesian inference*.
One of the main reasons for this rising interest is that Bayesian inference allows to quantify evidence against an assumed null hypothesis, but also to yield quantitative evidence in favor the null hypothesis.
Unfortunately, there are several approaches to hypothesis testing within the Bayesian framework, and many of them are either conceptually challenging, computationally too costly, or both.
For example, there are good conceptual arguments that support Bayesian hypothesis testing through *model comparison* using Bayes factors [@KassRaftery1995-Bayes-Factors;@VandekerckhoveMatzke2013-Model-Compariso;@MoreyRomeijn2016-philosophyOfBFs], but the computation of Bayes factors can be quite costly, especially for complex models.
Yet, for some of the most common use cases, there are some simple and computationally cheap approaches to Bayesian hypothesis testing with Bayes factors that are easy to understand and implement.
One such method is the *Savage-Dickey density ratio* [@DickeyLientz1970-The-Weighted-Li;@WagenmakersLodewyckx2010-Bayesian-hypoth].
While prior work has prominently documented how to use this method for the case of point-valued null-hypotheses [@WagenmakersLodewyckx2010-Bayesian-hypoth], this method can be hard to estimate reliably with posterior sampling, the most prevalent metod for approximate Bayesian computation at the moment. 
This tutorial therefore focuses on the use of the Savage-Dickey density ratio for testing hypotheses that are grounded in *regions of practical equivalence* (ROPEs) [@kruschke_Rejecting_journalarticle_2018] using the so-called *encompassing priors* approach [@KlugkistKato2005-Bayesian-model;@KlugkistHoijtink2007-The-Bayes-facto;@Oh2014-Bayesian-compar;@WetzelsGrasman2010-An-encompassing], which is both conceptually more meaningful and computationally more robust than point-valued hypothesis testing.
In sum, this tutorial provides an accessible, non-technical introduction to Bayesian hypothesis testing that is easy to understand, computationally cheap and widely applicable (though not universally).



# Motivation and intended audience

This tutorial provides a very basic introduction to the topic using R (R Core Team, 2025).
We wrote this tutorial with a particular reader in mind.
If you have used R before and if you have a basic understanding of linear regression, and Bayesian inference, this tutorial is for you.
We will remain mostly conceptual to provide you with a conceptual tool to approach hypothesis testing within Bayesian inference.
The form of hypothesis testing that we would like to introduce to you is, however, different from the traditional null hypothesis significance testing in that it requires more thinking about the quantitative nature of your data.
This is not a bug but, at least for us, a feature that will allow you to understand both your data and what you can learn from them better.

If you don’t have any experience with regression modeling, you will probably still be able to follow, but you might also want to consider doing a crash course.
To bring you up to speed, we recommend the excellent tutorial by Bodo @winter_Linear_preprint_2013 on mixed eﬀects regression in a non-Bayesian —a.k.a.
frequentist—paradigm.
To then make the transition to Bayesian versions of these regression models, we shamelessly suggest our own tutorial on "Bayesian Regression for Factorial Designs" as a natural follow-up using the same data that Winter used [@franke-roettger_Bayesian_preprint_2019].
In a sense, the present tutorial on hypothesis testing could be considered the long-awaited sequel of the series started by Winter.
For continuity, we will continue to use the original data set.

To actively follow this tutorial, you should have R installed on your computer (https://www.r-project.org).
Unless you already have a favorite editor for tinkering with R scripts, we recommend to try out RStudio (https://www.rstudio.com).
You will also need some packages, which you can import with the following code:

```{r setup}
#| echo: FALSE

# set the random seed in order to make sure
# you can reproduce the same results
set.seed(1702)

# project colors
project_colors = c(
  "#7581B3", "#99C2C2", "#C65353", "#E2BA78", "#5C7457", "#575463",
  "#B0B7D4", "#66A3A3", "#DB9494", "#D49735", "#9BB096", "#D4D3D9",
  "#414C76", "#993333"
  )

```

```{r libraries}
#| message: FALSE

# package for convenience functions (e.g. plotting)
library(tidyverse)
library(ggdist)

# package for Bayesian regression modeling
library(brms)

# package for posterior wrangling and plotting
library(tidybayes)

# package for BF calculation and plotting
library(bayestestR)

options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())

```

# Data, research questions & hypotheses

This tutorial looks at a data set relevant for investigating whether voice pitch diﬀers across social contexts in Korean.
Korean is a language in which the social distance between speakers plays a central role.
The way Korean speakers speak depends for example on whether they are in a formal context (e.g. during a consultation with a professor) or an informal context (e.g. chatting with a friend about the holidays) [@winter-grawunder_Phonetic_journalarticle_2012].

To load the data into your R environment, run the following code

```{r load-data}
#| message: FALSE

# TO DO: STORE ONLINE
# TO DO: SIMPLIFY STORED DATA
polite = read_csv("../data/polite.csv") |> 
  # remove men
  filter(gender == "F") |> 
  # transform context to factor
  mutate(context = as.factor(context))

polite

```

This data set contains anonymous identifiers for individual speakers stored in the variable `subject.` 
In this tutorial we will only be looking at the data from female speakers.
Subjects produced diﬀerent sentences, and the experiment manipulated whether the sentences were produced in a `formal` or an `informal` social context, indicated by the variable `context.` 
Crucially, each row contains a measurement of pitch in Hz stored in the variable `pitch`.

For most analyses of behavioral experiments, researchers are interested in whether an outcome variable is meaningfully affected by at least one manipulated variable and if so how the outcome variable is affected by it.
In this case, @winter-grawunder_Phonetic_journalarticle_2012 wanted to test whether pitch is meaningfully affected by the social context of the utterance.

As a first step, we can explore this question visually.
@fig-descriptive-dataviz displays the pitch values for all utterances in the dataset across contexts (semi-transparent points).
The solid points indicate the average pitch values across all sentences and speakers.
Looking at the plot, we can see that voice pitch from utterances in formal contexts are on average slightly lower than those in informal contexts.
The red distribution is slightly shifted to the left of the blue distribution by around 1.3 semitones.
In other words, speakers tend to slightly lower their voice pitch when speaking in a formal context.
But there is also a lot of overlap between the two contexts.
Now as Bayesians, we would like to translate the data into an expression of evidence: does the data provide evidence for our research hypotheses?

```{r descriptive-dataviz}
#| message: FALSE
#| warning: FALSE
#| echo: FALSE
#| fig-height: 4
#| fig-cap: "Empirical distribution of female speakers' pitch values across contexts"
#| fig-pos: "!tbph"
#| label: fig-descriptive-dataviz

polite |> 
  # aggregate mean values for context
  group_by(context) |> 
  summarize(pitch = mean(pitch, na.rm = TRUE)) |> 
  ggplot(aes(y = context, 
             x = pitch, 
             fill = context,
             colour = context)) + 
  stat_histinterval(data = polite,
                    position = "identity", 
                    alpha = 0.5,
                    color = NA,
                    breaks = seq(floor(min(polite$pitch, na.rm = T)) - 7, 
                                 ceiling(max(polite$pitch, na.rm = T)), 
                                 by = 10), 
                    outline_bars = FALSE) +
  # plot all data as semitransparent points
  geom_point(data = polite,
             position = position_dodge(0.5), 
             alpha = 0.5, 
             size = 3) +
  # plot mean values per condition as large points
  geom_point(position = position_dodge(0.5), 
             pch = 21, 
             colour = "black",
             size = 5) +
  #scale_x_continuous(limits = c(10,40)) +
  scale_colour_manual(breaks = c("informal", "formal"),
                      values = c(project_colors[1], project_colors[3])) +
  scale_fill_manual(breaks = c("informal", "formal"),
                      values = c(project_colors[1], project_colors[3])) +
  labs(x = "\npitch in Hz",
       y = "social context\n") +
  theme_minimal() +
  theme(legend.position = "none")


```



Let us build a Bayesian linear model to approach an answer to this question.
Using the package `brms` [@Burkner2018-Advanced-Bayesi], our first step is to specify the model formula and check which priors need to be specified:

```{r model_prep}
#| output: false

# contrast code predictor

contrasts(polite$context) <- c(-0.5,0.5)

# define linear model formula
# predict pitch by context and allow for that relationship 
# to vary between subjects
formula <- bf(pitch ~ context + (1 + context | subject))

# get priors for this model
get_prior(formula, polite)

```

The default priors that `brms` picks for the Intercept and the variance parameters are mostly reasonable as they are derived from the data, weakly informative and symmetrical.
However the prior for our critical parameter `context1` should also be weakly informative [@gelman-etal_Prior_journalarticle_2017], i.e. the prior assumption about the difference between informal and formal contexts should be that we don't know, but our best guess is that it is close to zero and equally likely to be more or less than zero.
So we specify a normal distribution centered on zero for this parameter.
(NB: We use default priors for the other parameters for convenience here, but you always should critically reflect on all of your priors.)

```{r priors-2}

# pick a weakly informative prior for the critical parameter
priors <- prior(normal(0, 20), 
                class = b, 
                coef = "context1")

```

Now we do a so-called prior predictive check, in other words we want to know what the posterior distribution looks like before having seen the data, based on the priors only.
This is a useful exercise to make sure that the priors results in reasonable quantitative assumptions.
We usually do it for all parameters, but here we will focus only on the critical parameter `context1`, i.e. the difference between formal and informal contexts.
Let us also have a look at the predictions for the prior-only model.

```{r priors-model}
#| warning: FALSE
#| message: FALSE

# NOTE: CAN WE STORE THE SAMPLING PARAMETERS 
#       (seed, iter, chains, cores, backend, data)? 
#       TO MAKE THE CODE CHUNKS SMALLER?
# only for 'cores' and 'backend', but not for seed, iter, chains.
# the the latter, we could define a custom brms-function that bakes these
# defaults in


# run the model
fit_prior <- brm(formula,
           prior = priors,
           family = gaussian(),
           # sample prior only
           sample_prior = "only",
           # store / load model output
           file  = "../models/fit_prior",
           # common sampling specifications
           seed = 1234,
           iter = 8000,
           chains = 4,
           data = polite)
           
```

```{r plot-priors}
#| fig-height: 4
#| fig-cap: "Prior probability of the effect of context on pitch, i.e. before seeing the data"
#| label: fig-plot-priors


# extract prior samples
prior_samples <- 
  fit_prior |> 
  spread_draws(b_context1)
  
# plot  
ggplot(prior_samples,
       aes(x = b_context1)) + 
  stat_histinterval(slab_color = project_colors[11],
                    slab_fill = alpha(project_colors[11], 0.5),
                    fill = NA,
                    color = NA,
                    outline_bars = FALSE) +
  labs(x = "\n prior pitch difference between formal and informal contexts",
       y = "") +
  scale_x_continuous(limits =c(-70,70)) +
  theme_minimal() +
  theme(axis.text.y = element_blank())

```

Looking at the distribution in @fig-plot-priors, the priors for the effect of context on pitch seems sensible.
The most plausible value is zero.
Values that are smaller or larger than zero become less plausible the further they are away from zero and values being smaller or larger than zero are equally likely.
Good.
Before we have seen the data, our model is somewhat pessimistic about the effect of context on on pitch.
Now we can run the full model that integrates the likelihood (our data) with the priors and visualize the posteriors for the critical parameter.

```{r model-1}
#| warning: FALSE
#| message: FALSE
#| output: FALSE

# run the model
fit <- brm(formula,
           prior = priors,
           family = gaussian(),
           # store / load model output
           file  = "../models/fit",
           # common sampling specifications
           seed = 1234,
           iter = 8000,
           chains = 4,
           data = polite)
           
```

```{r plot-posterior}
#| warning: FALSE
#| message: FALSE
#| fig-height: 4
#| fig-cap: "Posterior probability of the effect of context on pitch, i.e. after seeing the data"
#| label: fig-plot-posterior


# plot prior and posterior
posterior_plot <- fit |> 
  spread_draws(b_context1) |> 
  ggplot(aes(x = b_context1)) + 
    stat_histinterval(data = prior_samples,
                     slab_color = project_colors[11],
                     slab_fill = alpha(project_colors[11], 0.5),
                     fill = NA,
                     color = NA,
                     outline_bars = FALSE) +
    stat_histinterval(slab_color = project_colors[14],
                      slab_fill = alpha(project_colors[14], 0.5),
                      color = NA,
                      outline_bars = FALSE) +
  scale_thickness_shared() +
  labs(x = "\n pitch difference between formal and informal contexts",
       y = "") +
  scale_x_continuous(limits =c(-70,70)) +
  theme_minimal() +
  theme(axis.text.y = element_blank())

posterior_plot
```

@fig-plot-posterior shows the prior (green distribution) and posterior (red distribution) probability of the effect of context on pitch.
The distribution of posterior samples (red) suggests that the majority of plausible values after seeing the data are positive, or in other words, informal contexts elicit larger pitch values.
Negative values are not very plausible posterior values, but also not completely implausible.
Compared to our prior probability (green distribution) for which roughly 50% of posteriors are negative, this decrease in plausibility of negative values is quite noteworthy already.

What we have done here should be quite familiar.
We compare our model predictions to a reference point.
It is a single point value: zero.
But do we really care that much for such point hypotheses?
Is zero really that special?
We might think so because years of using null hypothesis significance testing has conditioned us to think that way.
But this tutorial would like to break this cycle and move forward.
Bear with us and let's approach hypothesis testing a bit differently today.

## Grounding hypotheses in regions of practical equivalences

Above we claimed that we wanted to test "whether pitch is **meaningfully affected** by the social context of the utterance".
We snuck the word meaningfully in there for a reason.
But what does "meaningful" mean?
This is really a good questions and (un)fortunately requires quite a bit of thinking.
This tutorial deals with speech data.
Speech is, in spoken languages at least, THE vehicle to transmit linguistic information in order to communicate with each other.
Speech is also very complex and very noisy: Not everything that can be measured in the acoustic signal matters for the listener.
For example, if something cannot be perceived reliably, it is at least conceivable that it might play little to no role in communication.
While speech sciences has a rich research tradition to estimate what can and what cannot be reliably heard, exact estimates depends on a lot of moving parts.Such thresholds are referred to as Just Noticeable Differences (JNDs) and can be used to define what constitutes meaningful differences when we look at speech data.

For example, @liu2013just report on JNDs ranging from 3 to 14 Hz. @jongman2017just report on JNDs between 6 and 9 Hz. @turner2019perception reported on JNDs between 17 and 25 Hz for non-speech stimuli and between 35 and 40 Hz for speech stimuli. While these studies are hard to compare, they give us at least a the range of JND values to work with. 

So we could interpret the original hypothesis the following way: If a pitch difference is below the JND, it is not meaningful.
So instead of testing against a point-zero hypothesis, we can test against a range of parameter values that are equivalent to the null value for practical purposes.
In our case, let us begin with the lowest reported JND of the above studies on pitch perception in speech (3 Hz), but be extra conservative and double the reported JND to 6 Hz. We then assume that pitch values between `-6` and `6` are meaningless.
Such ranges are sometimes called regions of practical equivalence (ROPEs), range of equivalence, equivalence margin, smallest effect size of interest, or good-enough belt [see @kruschke_Rejecting_journalarticle_2018].

```{r rope}

rope <- c(-6,6)
  
```

With a ROPE being defined, we can now test our hypothesis "whether pitch is **meaningfully affected** by the social context of the utterance" using Bayes Factor:

# Testing hypothesis using Bayes Factor

## What is Bayes Factor

Bayes Factors (henceforth: BFs) allow us to quantify relative evidence of one model compared to another.
<!-- @Michael swoops in and dazzles the crowd with a crispy introduction without getting too nerdy -->

## Approximating Bayes Factor with Savage Dickey

## Calculating Bayes Factor for a specified Region of Practical Equivalence (ROPE)

Instead of doing it by hand, we can calculate the Savage Dickey ratio with the `bayesfactor_parameters()` function from the `bayesfactorR` package.
What happens behind the scenes is that the function will sample posteriors from your specified model based on priors only (so before seeing any data) and calculates the posterior probability of the specified `null` hypothesis (here the range specified by our ROPE).

```{r BF_rope}
#|warning: FALSE
#|message: FALSE

BF_1 <- bayesfactor_parameters(posterior = fit, 
                               null = rope, 
                               parameter = "b_context1")

```

Before interpreting the number we get, let us visually explore what our BF corresponds to.

```{r plot-ropes}
#| warning: FALSE
#| message: FALSE
#| fig-height: 4
#| fig-cap: "Prior and posterior probability of the effect of context on pitch relative to the ROPE (-0.1, 0.1)"

posterior_plot + 
  geom_vline(xintercept = c(rope[1], rope[2]),
             lty = "dashed")

```

What the BF does is relating two numbers: (a) The prior probability of parameter values outside the rope, i.e. the proportion of the green distribution that falls outside the dashed lines, and (b) the posterior probability of parameter values outside the rope, i.e. the proportion of the red distribution that falls outside the dashed lines.
Eye-balling the plot, we can maybe already see that more of the red distribution is outside the ROPE than of the green distribution.

```{r BF_rope_print}

BF_1

```

To be exact, 2.7 times for of the red distribution is outside of the ROPE than of the green distribution.

That means the model that has seen the data provide 2.7 times more evidence for pitch being outside of the ROPE, or in other words, it is 2.7 times more likely (after having seen the data), that context affects pitch meaningfully.
According to @lee-wagenmakers_Bayesian_book_2014 criteria for interpreting BFs, this value corresponds to only anecdotal evidence for the alternative hypothesis.

## Sensitivity analysis for different priors and ROPEs

Now as you probably have guessed already, all these probabilities are very much dependent on the priors of the model, so it is important to evaluate the robustness of our Bayes Factor-based interpretation across a range of sensible priors.
And as long as we are not a 100% sure about what a meaningful difference is, we might as well explore the robustness of the Bayes Factor across different ROPEs.
We won't bore you with the code for that process, but you can follow it along in our scripts.
Let us explore the following ROPE intervals as informed by the three studies cited above on pitch perception: we test a range of ROPE intervals from 6 Hz to 40 Hz.
We also assume the following five prior values for the width of the standard deviation of the critical parameter (centered on zero): 10, 15, 20, 25, 30.
These are all sensible prior widths assuming that medium to strong effects in either direction are plausible.

```{r loop-priors-for-full-models}
#| echo: FALSE
#| message: FALSE

# whether to rerun or load stored models
rerun <- FALSE

if (rerun) {
  
  # define 5 different priors widths that make sense
  priors_10 <- prior(normal(0, 10), class = b, coef = "context1")
  priors_15 <- prior(normal(0, 15), class = b, coef = "context1")
  priors_20 <- prior(normal(0, 20), class = b, coef = "context1")
  priors_25 <- prior(normal(0, 25), class = b, coef = "context1")
  priors_30 <- prior(normal(0, 30), class = b, coef = "context1")
  
  # Define a list of these prior specifications
  prior_list <- list(priors_10,
                     priors_15,
                     priors_20,
                     priors_25,
                     priors_30
  )
  
  # # Initialize a list to store models
  model_list <- list()
  
  # Loop over the priors
  for (i in seq_along(prior_list)) {
      model_list[[i]] <-
        brm(
          formula = formula,
          data = polite,
          prior = prior_list[[i]],
          # common sampling specifications
          seed = 1234,
          iter = 8000,
          chains = 4,
          cores = 4,
          backend = "cmdstanr"
    )
  }
  
  # name models
  names(model_list) <- paste0("xmdl_prior_", c(10,15,20,25,30))
  
  # store models
  saveRDS(model_list, "../models/model_loop.RDS")
  
} else {
  model_list <- readRDS("../models/model_loop.RDS")
 }


```


```{r loop-through-ropes}
#| echo: FALSE
#| message: FALSE

# specify different rope intervals

# Generate sequence of lower bounds
#lower_bounds <- seq(6, 40, by = 2)

# Create list of value pairs
#ropes <- lapply(lower_bounds, function(x) c(x, -x))
 
# results <- map_dfr(names(model_list_all), function(model_name) {
#   model <- model_list_all[[model_name]]
# 
#   map_dfr(ropes, function(interval) {
#     lower <- interval[1]
#     upper <- interval[2]
# 
#     bayesfactor_parameters(
#       model,
#       null = c(lower, upper),
#       parameter = "b_context1"
#     ) %>%
#       as_tibble() %>%
#       mutate(
#         model = model_name,
#         ROPE = upper,
#         interval_range = paste0("[", lower, ", ", upper, "]")
#       ) %>%
#       select(model, everything())
#   })
# })

# wrangle
# results_wrangled <- results |>
#    filter(Parameter == "b_context1") |>
#    separate(model, sep = "_", into = c(NA, NA, "prior_sd")) |>
#    mutate(BF = exp(log_BF),
#           ROPE = abs(ROPE)) |>
#    select(prior_sd, BF, ROPE, interval_range)

#write_csv(results_wrangled, "../models/loop_results.csv")
results_wrangled <- read_csv("../models/loop_results.csv")

```

```{r visualize-raster}
#| echo: FALSE
#| fig-height: 8
#| fig-cap: "Bayes Factors for a range of priors and a range of ROPEs"

# plot raster
ggplot(results_wrangled,
       aes(x = as.factor(prior_sd),
           y = reorder(interval_range, -ROPE),
           fill = BF)) +
  geom_tile(colour = "grey") +
  geom_text(aes(label = round(BF,2)),
            size = 2.5) +
  scale_fill_gradient2(limits = c(0.03,10),
                       transform = "log",
                       midpoint = 1, 
                       mid = "white",
                       low = "#5C7457", 
                       high = "#FA8100",
                       n.breaks = 4,
                       breaks = c(0.00000,0.03,0.1,0.33,1,3,10),
                       labels = c("0",
                                  "0.03 > very strong for H0","0.1 > strong for H0",
                                  "0.33 > moderate for H0","1 = inconclusive",
                                  "3 < moderate for H1","10 < strong for H1"),
                       oob = scales::squish
                       ) +
  labs(title = "(A) Bayes factor in favour of alternative", 
       subtitle = "from moderate evidence against the null\nto very strong evidence for the null",
       fill = "Bayes Factor\n",
       y = "ROPE in Hz centered on 0 by 2 Hz steps\n",
       x = "\nprior standard deviation in Hz") +
  theme_minimal()

```

The combination of Bayes Factors is visualized in Figure X. Orange cells indicate evidence for the alternative.
Green cells indicate evidence for the null.
It becomes clear that the conclusions we can draw from our data are rather dependent on the choices we made along the way.

By comparing the Bayes Factors along the y-axis, we can see that they are heavily dependent on the chosen ROPE.
We here chose (theoretically speaking) a quite large range of ROPEs, all of which are informed by psychoacoustic studies of what pitch differences can be reliably heard and thus likely are meaningful for communication. 
In light of this range of possible definitions what constitutes meaningful differences, our data seem not very robust, as illustrated by the shift from orange to green. Even the smallest ROPE intervals provide only anecdotal to moderate evidence for the alternative. And the most conservative ROPEs, following @turner2019perception, leads to moderate to very strong evidence against the alternative hypothesis. 

Additionally, when comparing the Bayes Factors along the x-axis, we can see that they are comparatively consistent for different standard deviations of the critical prior.
However, we can also see that the Bayes Factors decrease with the width of the priors (from left to right).
This is not surprising and a known phenomenon, often discussed under the Jeffreys-Lindley paradox [@lindley_Statistical_journalarticle_1957]: The more diffuse the priors are (i.e. wider priors), the larger is the probability that a specific parameter values is not compatible with the data.

Combined, we can see that the larger the ROPE and the wider the priors, the more likely becomes the null hypothesis.
In an ideal world, the evidence provided by the data should be robust across these choices.
However, this exploration of our inference is a fantastic opportunity to assess the boundaries of our conclusions. In this case, the original conclusions by @winter-grawunder_Phonetic_journalarticle_2012 was based on the null hypothesis significance testing and traditionally tested the compatibility of the data with a point-null hypothesis. They concluded "that in formal speech, Korean [...] female speakers lowered their average fundamental frequency [...]." This statement is still true according to their inferential criteria, but thinking more deeply about the theoretical consequences of differences in pitch, it might be less clear that these differenecs are truly meaninful. 

## BF for point hypothesis

don't lol

# How to write things up

# Some words of encouragement
Bayesian inference in general and this form of hypothesis testing in particular require much more thinking than we might be used to. We think this is a good thing. Many voices have criticized the lack of engagement that we behavioral scientists invest into thinking how our theoretical ideas connect to concrete predictions in the quantitative systems under investigation [e.g. @scheel2022most; @coretta2023multidimensional; @woensdregt2024lessons]. The presented form of hypothesis testing is easy to understand, but does require to think deeply about prior quantitative assumptions as well as what it means for observations to be meaningfully different. That is neither trivial nor easy. But we would like to encourage you to engage in exactly this thinking to better understand our data and how they might link with our understanding of cognition and behavior.  

# Other Resources
There are many fantastic resources out there to help you learn about the wonderful world of statistics. Here are a few recommendations.
- A very accessible introduction to linear models in R is @winter2019statistics.
- ...


# References

```{r sessionInfo}
#| echo: FALSE

sessionInfo()

c("brms", "bayestestR", "tidybayes", "tidyverse", "ggdist") %>%
  map(citation) %>%
  print(style = "text")

```

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
